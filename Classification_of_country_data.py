# -*- coding: utf-8 -*-
"""Country_Data_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dQd-g0Y0Hqdn1Gd9BmraYI9ATlFvpjUh

## Country Data
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score,precision_score,recall_score
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

'''
Class Kmeans has the following methods:

__init__(self,no_clusters,co_ordinates,max_iterations) : 
Initializes the parameters for the K-means clustering algorithm

Train(self,X) : 
Clusters the data X using the K-means algorithm and returns the cluster centroids

predict(self,X) :
 Assigns each data point in X to the closest cluster centroid

new_clusters(self,classes,X) : 
Calculates the new cluster centroids using the mean of the data points assigned to each cluster

score(self,X,pred_classes) : 
Calculates the sum of squared errors between the predicted cluster centroids and the actual data points.
The initialization parameters are:

no_clusters : Number of clusters to be formed
co_ordinates : Array of the initial cluster centroids
max_iterations : Maximum number of iterations to be run for convergence.
'''
class Kmeans:
    '''Intialize the parameters as for K - means clustering'''
    def __init__(self,no_clusters,co_ordinates,max_iterations):
        self.no_clusters = no_clusters
        self.co_ordinates = np.array(co_ordinates)
        self.max_iterations = max_iterations
    '''Cluster the data'''
    def Train(self,X):
        X = np.array(X)
        classes = []

        # TO Check if there is any significant difference between previous two co_ordinates
        flag = True

        # Iterate till max_iterations is reached
        for i in range(self.max_iterations):

            classes = self.predict(X)
            flag = self.new_clusters(classes,X)

            # If there is no significant difference between previous two co_ordinatess return co_ordinates
            if(flag):
                continue
            else:
                break
        return self.co_ordinates

    def predict(self,X):
        X = np.array(X)
        # Assign each data point to the closest cluster center
        clusters = np.zeros(X.shape[0])
        for i, x in enumerate(X):
            distances =np.sqrt(np.sum((x - self.co_ordinates)**2, axis=1))
            cluster = np.argmin(distances)
            clusters[i] = cluster
        return clusters.astype(np.int32)

    '''change cluster to the mean of data points of that cluster'''
    def new_clusters(self,classes,X):

        old_cor = self.co_ordinates
        for i in range(0,self.no_clusters):

            # Iterate over each  claster
            X_c = X[classes == i]
            if(len(X_c)>10):
                self.co_ordinates[i,:] = np.mean(X_c, axis=0)
            else:

            # If there are no points in that cluster assign a new random point from the give datapoints
                self.co_ordinates[i,:] =  X[np.random.choice(len(X), 1 ,replace= False)]
        
        # If the co_ordinates are close enough
        if np.allclose(old_cor,self.co_ordinates,rtol=1e-1):
            return False
        else:
            return True
        
        '''Calculate the sum of squared error'''
    def score(self,X,pred_classes):
        dist = 0
        for c in np.unique(pred_classes):
            X_c = X[ pred_classes == c ]
            for sample in X_c:
                dist += np.linalg.norm(sample-self.co_ordinates[int(c),:])**2
        return dist

df  = pd.read_csv("Country-data.csv")
df.head()
# df.iloc[34]

country_names = df.country
df.drop(['country'],axis = 1,inplace=True)
df

df.shape

'''Get the information of the data(null_values,data_type,columns)'''
df.info()

df.isna().sum()

"""Hence we can say that no null values are present the dataset"""

df.describe()

''' Find the correlated values in the given dataset '''
plt.figure(figsize=(10,5))
cor = df.corr()

'''finding correlation using heatmap by seaborn'''

sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, annot_kws={"fontsize":15})
plt.xticks(fontsize=17)
plt.yticks(fontsize=17)
plt.show()

"""GDPP and Income are highly correlated ,total fertility and child mortality are highly correlated

## K-means implementation
"""

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, max_iter=300, random_state=0)
    kmeans.fit(df)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.grid()
plt.show()

silhouette_scores=[]
for i in range(2,11):
    model = KMeans(n_clusters=i)
    model.fit(df)
    labels=model.predict(df)
    # print(f'Silhouette Score(n={i}): {silhouette_score(X, labels)}')
    silhouette_scores.append(silhouette_score(np.array(df), labels))
silhouette_scores=np.array(silhouette_scores)
plt.plot(range(2,11),silhouette_scores)
plt.xlabel("No_of_clusters")
plt.ylabel("Silhouette_score")
plt.title("Silhouette_scores")
plt.grid()
plt.show()

"""By looking at the graphs of the two methods, since there's a sharp change at k=3 after that we have a gradual change hence we can choose the value of k to be 3

"""

'''Initalize centres from outside 
    df.iloc[159] is america which is the that doesn't require any finiancial assistance
    df.iloc[18] is bhutan which is poorest country in the world and will require financial assistance
    df.iloc[34] is china which will require very little effort.
    Now, we can cluster our data according to our analysis

'''
init_centre = np.array([np.array(df.iloc[18]),np.array(df.iloc[158]),np.array(df.iloc[69])])


np.array(df.iloc[125])
np.array(df.iloc[18])
np.array(df.iloc[159])
# country_names[]

model = KMeans(n_clusters= 3,max_iter=1000,init=init_centre,n_init=1)
model.fit(df)
model.cluster_centers_
labels = model.labels_
labels

def unique_indices(arr):
    indices = {}
    for i, elem in enumerate(arr):
        if elem in indices:
            indices[elem].append(i)
        else:
            indices[elem] = [i]
    return indices

uq = unique_indices(labels)
country_names.iloc[uq[0]]

country_names.iloc[uq[1]]

country_names.iloc[uq[2]]

df.head()

from mpl_toolkits.mplot3d import Axes3D
centroids = np.array(model.cluster_centers_)
labels = model.labels_
df['Class'] = labels

fig = plt.figure()
ax = Axes3D(fig)
x = np.array(df['gdpp'])
y = np.array(df['total_fer'])
z = np.array(df['life_expec'])
ax.scatter(centroids[:,0],centroids[:,1],centroids[:,2],marker="X", color = 'r')
ax.scatter(x,y,z,c = y)
plt.title('Gdpp vs total_fer vs life_expec')
ax.set_xlabel('Gdpp')
ax.set_ylabel('total_fer')
ax.set_zlabel('life_expec')
plt.show()

"""We have our plots but we don't know which cluster centre belongs to which category.

So we will have to visualize our data inorder to draw conclusions from it.

Since we know that low GDP per capita is a sign of a econmocially backward nation, we can draw a box plot to see which cluster belongs to which category.

We know that under - developed countries which includes mostly african countries tend to have high fertility rate, and as country develops the standered of living increases and total fertility rate also reduces as parents prefer to have less children.

A high life expectency will mean that the country spends more on health and people are generally halthier which happens in mostly rich countries.
"""

sns.boxplot(x="Class", y="gdpp", data=df)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('gdpp')
plt.title('GDPP vs Classes')

# show the plot
plt.show()

"""Using GDPP vs Class plot we can see that the classe 1 is the welthiest country in term of economic categorization.
Class 0 perfroms the worst in our analysis hence it is in much more dire need of funds as compared to classes 0 and 2.
"""

sns.boxplot(x="Class", y="total_fer", data=df)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('total_fer')
plt.title('total_fer vs Classes')

# show the plot
plt.show()

"""As we mentioned earlier the coutries with higher per fertility are the coutries which are developing, from the plot we can conclude that class 0 is more socially backward as compared to other two classes, which is consitent with our previous analysis."""

sns.boxplot(x="Class", y="life_expec", data=df)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('life_expec')
plt.title('life_expec vs Classes')

# show the plot
plt.show()

"""As we mentioned earlier countries with lower life expectancy are poor contries who don't invest in the developing their infrastructure and medical facilities, hence the quality of the health of citizens decline and hence they have lower life expectancy.
Hence we can say that the class 0 requires the funds based on health as a factor, which is also consistent with our earlier analysis of gdpp and total fertility

Hence, we can conclude that,

Class 0 : Help Required

Class 1 : No Help Required

Class 2 : Little Help Required
"""

import plotly.express as px

df['country'] = country_names
df.Class.iloc[uq[2]] = 'Help Required'
df.Class.iloc[uq[1]] =  'No help Required'
df.Class.iloc[uq[0]] = 'Little help Required'

df.head()

# !pip install plotly

# create a choropleth map using the plotly express function
fig = px.choropleth(df, locations='country', locationmode='country names', color='Class',
                                        color_discrete_map = {'Help Required':'Red',
                                        'No Help Required':'Green',
                                        'Little help Required':'Yellow'}, title='Nations Requiring Help')
                    

# display the map
fig.show(renderer = "browser")

"""## KMeans using PCA

Principle Component Analysis is a technique that reduces the dimention of the dataset into consideration. It reduces the number od features in such way, so that the highest variance features are kept. The highest variance features give us the highest information.
Typically, the feature which explain at least 95% of variance is kept.
"""

df2  = pd.read_csv("Country-data.csv")
df2.head(5)

df2.drop(["country"],axis=1,inplace=True)
columns = df2.columns

"""Scaling the data to work with PCA."""

columns = df2.columns
for column in columns:
    df2[column] = df2[column]/df2[column].mean()
for column in columns:
    df2[column] = df2[column]/df[column].std()
df2

pca = PCA()
pca_df = pca.fit_transform(df2)
pca_df = pd.DataFrame(pca_df,columns= columns)
pca_df

evr = pca.explained_variance_ratio_

prev = 0
for i in range(0,len(evr)):
    evr[i] += prev
    prev = evr[i]

# plt.step(list(range(1,10))
plt.plot(range(1, len(evr)+1), evr)
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Percentage of Variance Explained')
plt.title('PCA Results: Cumulative Percentage of Variance Explained')
plt.step(np.arange(2,11,1),evr)
plt.grid()
plt.show()

"""We have plotted the culmelative ration of variance explained.

Typically eigen values with more than 95% of ratio of variance are selected.

They correspond to the columns of the PCA generated dataframe.

In this case, we select the number of principle components to be equal to 3, as it explains 95% of the variance
"""

pca_df2 = pca_df.drop(['inflation', 'life_expec', 'total_fer', 'gdpp','imports','income'], axis=1)
pca_df2

init_centre = np.array([np.array(pca_df2.iloc[18]),np.array(pca_df2.iloc[158]),np.array(pca_df2.iloc[69])])
np.array(pca_df2.iloc[125])
np.array(pca_df2.iloc[18])
np.array(pca_df2.iloc[159])

model = KMeans(n_clusters= 3,max_iter=1000,init=init_centre,n_init=1)
model.fit(pca_df2)
model.cluster_centers_
labels = model.labels_
labels

def unique_indices(arr):
    indices = {}
    for i, elem in enumerate(arr):
        if elem in indices:
            indices[elem].append(i)
        else:
            indices[elem] = [i]
    return indices

uq = unique_indices(labels)

from mpl_toolkits.mplot3d import Axes3D
centroids = np.array(model.cluster_centers_)
labels = model.labels_
pca_df2['Class'] = labels

fig = plt.figure()
ax = Axes3D(fig)
x = np.array(pca_df2['child_mort'])
y = np.array(pca_df2['exports'])
z = np.array(pca_df2['health'])
ax.scatter(centroids[:,0],centroids[:,1],centroids[:,2],marker="X", color = 'r')
ax.scatter(x,y,z,c = y)
plt.title('child_mor vs exports vs health')
ax.set_xlabel('child_mort')
ax.set_ylabel('exports')
ax.set_zlabel('health')
plt.show()

"""We will again check the box plot between the for different features, to see which country is in need of help."""

sns.boxplot(x="Class", y="child_mort", data=pca_df2)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('child_mort')
plt.title('child_mort vs Classes')

# show the plot
plt.show()

"""Class 2 has the highest child mortality and hence it is backward from a economic and health perspective.

Class 0 has less child mortality hence it is in less need of help as compared to class 2. 

Class 1 has least child mortality hence it doesn't require any help. 
"""

sns.boxplot(x="Class", y="health", data=pca_df2)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('health')
plt.title('health vs Classes')

# show the plot
plt.show()

"""Most countries in the class 2 lie below median hence most of them don't spend money on health, hence from a health perspective we should invest in the 2nd class of country.

The class 0 countries perform slightly better as compared to class 2 countries, hence need very little help.

The class 1 countries are in better conditions as compared to other classes.
"""

sns.boxplot(x="Class", y="exports", data=pca_df2)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('exports')
plt.title('exports vs Classes')

# show the plot
plt.show()

"""A high export would mean that the country is more self relaint and it doesn't require any external help.
Also, it's currency also gets more powerful.

The box plot of all the classes is nearly similar, this must be beacause we mean standerdized our data hence the differences also got smaller.

But carefully looking at the plot we can see that class 1 has slightly high export(median) as compared to other two countries hence it doesn't require any help.

Exports suggests that class 0 requires help but child mortality and health say that class 0 requires most help.

Also child mortality explains majority of variance in our data, hence we are going to conclude that class 0 requires the most help
"""

pca_df2

pca_df2['country'] = country_names
pca_df2.Class.iloc[uq[2]] = 'Help Required'
pca_df2.Class.iloc[uq[1]] =  'No help Required'
pca_df2.Class.iloc[uq[0]] = 'Little help Required'
pca_df2

# !pip install plotly

# create a choropleth map using the plotly express function
fig = px.choropleth(pca_df2, locations='country', locationmode='country names', color='Class',
                                        color_discrete_map = {'Help Required':'Red',
                                        'No Help Required':'Green',
                                        'Little help Required':'Yellow'}, title='Nations Requiring Help')
                    

# display the map
fig.show(renderer = "browser")

"""## Our Kmeans"""

df3  = pd.read_csv("Country-data.csv")
df3.head(3)

country_names = df3.country
df3.drop(['country'],axis = 1,inplace=True)
df3

init_centre = np.array([np.array(df3.iloc[18]),np.array(df3.iloc[158]),np.array(df3.iloc[69])])
np.array(df3.iloc[34])
np.array(df3.iloc[18])
np.array(df3.iloc[159])

model2 = Kmeans(no_clusters=3, co_ordinates=init_centre,max_iterations=100)
cluster_centres = model2.Train(np.array(df3))
labelss = model2.predict(np.array(df3))

cluster_centres

label_indices =  unique_indices(labelss)
country_names.iloc[label_indices[0]]

country_names.iloc[label_indices[1]]

country_names.iloc[label_indices[2]]

from mpl_toolkits.mplot3d import Axes3D
centroids = np.array(cluster_centres)

df3['Class'] = labelss

fig = plt.figure()
ax = Axes3D(fig)
x = np.array(df3['gdpp'])
y = np.array(df3['total_fer'])
z = np.array(df3['life_expec'])
ax.scatter(centroids[:,0],centroids[:,1],centroids[:,2],marker="X", color = 'r')
ax.scatter(x,y,z,c = y)
plt.title('Gdpp vs total_fer vs life_expec')
ax.set_xlabel('Gdpp')
ax.set_ylabel('total_fer')
ax.set_zlabel('life_expec')
plt.show()

sns.boxplot(x="Class", y="gdpp", data=df3)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('gdpp')
plt.title('GDPP vs Classes')

# show the plot
plt.show()

"""Class 2 clearly has the least gdpp hence it requires more help from outside.

Class 1 needs doesn't need any help it has the highest gdpp.

Class 0 needs very little help.
"""

sns.boxplot(x="Class", y="total_fer", data=df3)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('total_fer')
plt.title('total_fer vs Classes')

# show the plot
plt.show()

"""Class 2 has the highest fertility, hence it's socially backwards and has a low standered of living.

Class 1 has lowest hence it doesn't need any help.

Class 0 has low fertility as compared to class 2, hence the class 0 countries require less help.
"""

sns.boxplot(x="Class", y="life_expec", data=df3)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('life_expec')
plt.title('life_expec vs Classes')

# show the plot
plt.show()

"""As we mentioned earlier countries with lower life expectancy are poor contries who don't invest in the developing their infrastructure and medical facilities, hence the quality of the health of citizens decline and hence they have lower life expectancy.

Hence we can say that the class 0 requires the funds based on health as a factor, which is also consistent with our earlier analysis of gdpp and total fertility
"""

df3['country'] = country_names
df3.Class.iloc[uq[2]] = 'Help Required'
df3.Class.iloc[uq[1]] =  'No help Required'
df3.Class.iloc[uq[0]] = 'Little help Required'

# !pip install plotly

# create a choropleth map using the plotly express function
fig = px.choropleth(df, locations='country', locationmode='country names', color='Class',
                                        color_discrete_map = {'Help Required':'Red',
                                        'No Help Required':'Green',
                                        'Little help Required':'Yellow'}, title='Nations Requiring Help')
                    

# display the map
fig.show(renderer = "browser")

"""## Our PCA """

# function to calculate the covariance matrix of a given matrix X
def cov(X):
    return X.T @ X

# function to perform principal component analysis on a pandas DataFrame A
def PCA(A , n_components):
    
    # get the column names of the input DataFrame A
    columns_A = A.columns
    
    # convert the DataFrame A into a NumPy array
    A = np.array(A)
    
    # calculate the mean and standard deviation of each column
    M = np.mean(A.T, axis=1)
    S = np.std(A.T, axis=1)
    
    # standardize the input data by subtracting the mean and dividing by the standard deviation
    C = (A - M) / S
    
    # calculate the covariance matrix of the standardized data
    V = cov(C)
    
    # calculate the eigenvalues and eigenvectors of the covariance matrix
    values, vectors = np.linalg.eig(V)

    # sort the eigenvalues and eigenvectors in descending order of their values
    sorted_indices = np.argsort(values)[::-1]
    values = values[sorted_indices]
    vectors = vectors[:, sorted_indices]
    columns_A = columns_A[sorted_indices]

    # calculate the principal components of A by multiplying the standardized data with the eigenvectors
    P = np.dot(C, vectors) 
    
    # select the first n_components principal components
    P = P[:,:n_components]
    
    # calculate the variance of each principal component
    P_std = np.var(P.T, axis=1)
    
    # create a new DataFrame containing the first n_components principal components and their corresponding column names
    df = pd.DataFrame(P, columns= columns_A[:n_components])
    
    # return the principal components DataFrame, the variances of each component, the first n_components eigenvectors, and the corresponding eigenvalues
    return df, P_std, vectors[:,:n_components], values

df3 = pd.read_csv("Country-data.csv")
df3

df3.drop(['country'],axis=1,inplace=True)

columns = df3.columns
# for column in columns:
#     df3[column] = df3[column]-df3[column].mean()
for column in columns:
    df3[column] = df3[column]/df[column].std()
mms_df3 = df3
mms_df3

pca_df3,std,vectors,values = PCA(mms_df3,n_components=9)
pca_df3

per_of_var = []
tot_val = np.sum(values)
for i in range(len(values)):
    per_of_var.append(values[i]/tot_val)
per_of_var = np.cumsum(per_of_var)

# plt.step(list(range(1,10))
plt.plot(range(1, len(per_of_var)+1), per_of_var)
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Percentage of Variance Explained')
plt.title('PCA Results: Cumulative Percentage of Variance Explained')
plt.step(np.arange(2,11,1),per_of_var)
plt.grid()
plt.show()

pca_df4 = pca_df3.drop(['inflation', 'life_expec', 'total_fer', 'gdpp'], axis=1)
pca_df4

init_centre = np.array([np.array(pca_df4.iloc[18]),np.array(pca_df4.iloc[158]),np.array(pca_df4.iloc[69])])
np.array(pca_df4.iloc[34])
np.array(pca_df4.iloc[18])
np.array(pca_df4.iloc[159])

model = Kmeans(no_clusters=3, co_ordinates=init_centre, max_iterations=1000)
cluster_centres2 = model.Train(pca_df4)
labelss2 = model.predict(pca_df4)

pca_df4

from mpl_toolkits.mplot3d import Axes3D
centroids = np.array(cluster_centres2)

pca_df4['Class'] = labelss2

fig = plt.figure()
ax = Axes3D(fig)
x = np.array(pca_df4['child_mort'])
y = np.array(pca_df4['health'])
z = np.array(pca_df4['income'])
ax.scatter(centroids[:,0],centroids[:,1],centroids[:,2],marker="X", color = 'r')
ax.scatter(x,y,z,c = y)
plt.title('child_mort vs health vs income')
ax.set_xlabel('child_mort')
ax.set_ylabel('health')
ax.set_zlabel('income')
plt.show()

sns.boxplot(x="Class", y="child_mort", data=pca_df4)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('child_mort')
plt.title('child_mort vs Classes')

# show the plot
plt.show()

sns.boxplot(x="Class", y="health", data=pca_df4)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('health')
plt.title('health vs Classes')

# show the plot
plt.show()

sns.boxplot(x="Class", y="exports", data=pca_df4)

# set the axis labels
plt.xlabel('Classes')
plt.ylabel('exports')
plt.title('exports vs Classes')

# show the plot
plt.show()

pca_df4['country'] = country_names
pca_df4.Class.iloc[uq[2]] = 'Help Required'
pca_df4.Class.iloc[uq[1]] =  'No help Required'
pca_df4.Class.iloc[uq[0]] = 'Little help Required'
pca_df4

# create a choropleth map using the plotly express function
fig = px.choropleth(pca_df4, locations='country', locationmode='country names', color='Class',
                                        color_discrete_map = {'Help Required':'Red',
                                        'No Help Required':'Green',
                                        'Little help Required':'Yellow'}, title='Nations Requiring Help')
                    

# display the map
fig.show(renderer = "browser")

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
df  = pd.read_csv("/content/Country-data.csv")

country_names = df['country'] 
df = df.drop('country', axis=1)

from sklearn.preprocessing import StandardScaler
print(df)
#scaling of variables need to be done
scaler = StandardScaler()

# fit the scaler to the data and transform the data
scaled_data = scaler.fit_transform(df)

# create a new DataFrame with the scaled data
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)
# scaled_df['exports'] = scaled_df['exports'].apply(lambda x: x * 10)
# scaled_df['child_mort'] = scaled_df['child_mort'].apply(lambda x: x * 10)
# scaled_df['imports'] = scaled_df['imports'].apply(lambda x: x * 10)
# scaled_df['income'] = scaled_df['income'].apply(lambda x: x * 10)
# scaled_df['total_fer'] = scaled_df['total_fer'].apply(lambda x: x * 10)
# display the scaled DataFrame
print(scaled_df)

df_val = scaled_df.values

X = df_val

def euclidean_distance(x1, x2):
    """
    Calculate the Euclidean distance between two points.
    """
    return np.sqrt(np.sum((x1 - x2)**2))

class DBSCAN:
    """
    A class to perform DBSCAN clustering algorithm.
    """
    def __init__(self, eps=15, min_samples=5):
        self.eps = eps
        self.min_samples = min_samples

    def fit(self, X):
        """
        Fit the DBSCAN algorithm to the data.
        """
        # Initialize the labels and visited points.
        self.labels_ = np.zeros(X.shape[0])
        visited = np.zeros(X.shape[0], dtype=bool)

        # Initialize the cluster counter.
        cluster_count = 0

        # Iterate through all points in the data set.
        for i in range(X.shape[0]):
            # If this point has already been visited, continue to the next point.
            if visited[i]:
                continue

            # Mark the point as visited.
            visited[i] = True

            # Find all neighboring points within eps distance.
            neighbors = [j for j in range(X.shape[0]) if euclidean_distance(X[i], X[j]) < self.eps]

            # If there are less than min_samples neighboring points, mark as noise.
            if len(neighbors) < self.min_samples:
                self.labels_[i] = -1
                continue

            # Otherwise, start a new cluster.
            cluster_count += 1
            self.labels_[i] = cluster_count

            # Iterate through the neighboring points.
            for j in neighbors:
                # If this point has not been visited, mark as visited.
                if not visited[j]:
                    visited[j] = True

                    # Find all neighboring points within eps distance.
                    neighbors_prime = [k for k in range(X.shape[0]) if euclidean_distance(X[j], X[k]) < self.eps]

                    # If there are at least min_samples neighboring points, add to the current cluster.
                    if len(neighbors_prime) >= self.min_samples:
                        neighbors += neighbors_prime

                # If the point has not been assigned to a cluster yet, assign to the current cluster.
                if self.labels_[j] == 0:
                    self.labels_[j] = cluster_count

        return self.labels_




from sklearn.metrics import silhouette_score



# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores = []
labels_list = []
X = df_val
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=1.5, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X, labels)
    # Append silhouette score and labels to lists
    silhouette_scores.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 1.5')
plt.show()

# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores1 = []
labels_list1 = []
X1 = df_val
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=1.7, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X1)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X1, labels)
    # Append silhouette score and labels to lists
    silhouette_scores1.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores1)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 1.7')
plt.show()

# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores2 = []
labels_list2 = []
X2 = df_val
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=2.0, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X2)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X2, labels)
    # Append silhouette score and labels to lists
    silhouette_scores2.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores2)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 2.0')
plt.show()

# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores3 = []
labels_list3 = []
X3 = df_val
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=2.3, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X3)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X3, labels)
    # Append silhouette score and labels to lists
    silhouette_scores3.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores3)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 2.3')
plt.show()


# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores4 = []
labels_list4 = []
X4 = df_val
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=2.5, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X4)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X4, labels)
    # Append silhouette score and labels to lists
    silhouette_scores4.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores4)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 2.5')
plt.show()



# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores5 = []
labels_list5 = []
X5 = df_val
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=3.0, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X5)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X5, labels)
    # Append silhouette score and labels to lists
    silhouette_scores5.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores5)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 3.0')
plt.show()

# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores6 = []
labels_list6 = []
X6 = df_val
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=3.5, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X6)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X6, labels)
    # Append silhouette score and labels to lists
    silhouette_scores6.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores6)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 3.5')
plt.show()

import pandas as pd
from sklearn.decomposition import PCA

df3 = df
columns = df3.columns
for column in columns:
    df3[column] = df3[column]/df3[column].mean()
for column in columns:
    df3[column] = df3[column]/df[column].std()
mms_df3 = df3

import pandas as pd
from sklearn.decomposition import PCA

# Read data into a Pandas DataFrame
data = mms_df3

import pandas as pd
from sklearn.decomposition import PCA

def cov(X):
    return X.T @ X

def PCA(A , n_components):
    # M = np.mean(A.T,axis=1)
    columns_A = A.columns
    A = np.array(A)
    M = np.mean(A.T, axis=1)
    S = np.std(A.T, axis=1)
    # print(S)
    C = (A - M)/S
    V = cov(C)
    values , vectors = np.linalg.eig(V)

    sorted_indices = np.argsort(values)[::-1]
    values = values[sorted_indices]
    vectors = vectors[:, sorted_indices]
    columns_A = columns_A[sorted_indices]

    # values = values[:n_components]
    # vectors = vectors[:, :n_components]

    P =np.dot(C,vectors) 
    P = P[:,:n_components]
    P_std = np.std(P.T,axis=1)
    df = pd.DataFrame(P, columns= columns_A[:n_components])
    
    # Calculate variance of each component
    variance_ratios = values / np.sum(values)
    variances = values[:n_components]

    return df, P_std, vectors[:,:n_components], variance_ratios, variances

a,b,c,d,e = PCA(data, 5)
print(a)
print(d)
a['child_mort'] = a['child_mort'].apply(lambda x: x * 0.01)
a['exports'] = a['exports'].apply(lambda x: x * 0.1)
# Define range of min_samples values to try
min_samples_range = range(2, 20)

# Initialize empty lists to store silhouette scores and labels for each min_samples value
silhouette_scores6 = []
labels_list6 = []
X6 = a.values
# Loop over range of min_samples values and calculate silhouette scores
for min_samples in min_samples_range:
    # Initialize DBSCAN model with current min_samples value
    dbscan = DBSCAN(eps=3.5, min_samples=min_samples)
    # Fit model to data
    labels = dbscan.fit(X6)
    
    # Calculate silhouette score for current value of min_samples
    score = silhouette_score(X6, labels)
    # Append silhouette score and labels to lists
    silhouette_scores6.append(score)
    labels_list.append(labels)

plt.plot(min_samples_range, silhouette_scores6)
plt.xlabel('min_samples')
plt.ylabel('Silhouette score')
plt.title('Silhouette scores for different min_samples values for eps = 3.5')
plt.show()


def unique_indices(arr):
    indices = {}
    for i, elem in enumerate(arr):
        if elem in indices:
            indices[elem].append(i)
        else:
            indices[elem] = [i]
    return indices

uq = unique_indices(labels)
print(uq)
a['Class'] = labels
a['country'] = country_names


a.Class.iloc[uq[-1]] = 'Help Required'

a.Class.iloc[uq[1]] = 'Little help Required'




import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)




X7 = df_val
# Loop over range of min_samples values and calculate silhouette scores

# Initialize DBSCAN model with current min_samples value
dbscan = DBSCAN(eps=1.0, min_samples=2)
# Fit model to data
labels7 = dbscan.fit(X7)


uq = unique_indices(labels7)
print(uq)
a['Class'] = labels
a['country'] = country_names


a.Class.iloc[uq[-1]] = 'Help Required'

a.Class.iloc[uq[1]] = 'Little help Required'




import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)

X7 = df_val
# Loop over range of min_samples values and calculate silhouette scores

# Initialize DBSCAN model with current min_samples value
dbscan = DBSCAN(eps=1.5, min_samples=2)
# Fit model to data
labels7 = dbscan.fit(X7)


uq = unique_indices(labels7)
print(uq)
a['Class'] = labels
a['country'] = country_names


a.Class.iloc[uq[-1]] = 'Help Required'

a.Class.iloc[uq[1]] = 'Little help Required'




import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)

X7 = df_val
# Loop over range of min_samples values and calculate silhouette scores

# Initialize DBSCAN model with current min_samples value
dbscan = DBSCAN(eps=2.0, min_samples=5)
# Fit model to data
labels7 = dbscan.fit(X7)


uq = unique_indices(labels7)
print(uq)
a['Class'] = labels
a['country'] = country_names


a.Class.iloc[uq[-1]] = 'Help Required'

a.Class.iloc[uq[1]] = 'Little help Required'




import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)


X7 = df_val
# Loop over range of min_samples values and calculate silhouette scores

# Initialize DBSCAN model with current min_samples value
dbscan = DBSCAN(eps=3.5, min_samples=3)
# Fit model to data
labels7 = dbscan.fit(X7)


uq = unique_indices(labels7)
a['Class'] = labels
a['country'] = country_names


a.Class.iloc[uq[-1]] = 'Help Required'

a.Class.iloc[uq[1]] = 'Little help Required'




import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)


X7 = df_val
# Loop over range of min_samples values and calculate silhouette scores

# Initialize DBSCAN model with current min_samples value
dbscan = DBSCAN(eps=2.1, min_samples=6)
# Fit model to data
labels7 = dbscan.fit(X7)


uq = unique_indices(labels7)

a['Class'] = labels
a['country'] = country_names


a.Class.iloc[uq[-1]] = 'Help Required'

a.Class.iloc[uq[1]] = 'Little help Required'




import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)

X7 = df_val
# Loop over range of min_samples values and calculate silhouette scores

# Initialize DBSCAN model with current min_samples value
dbscan = DBSCAN(eps=10.0, min_samples=12)
# Fit model to data
labels7 = dbscan.fit(X7)


uq = unique_indices(labels7)
print(uq)
a['Class'] = labels
a['country'] = country_names







import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)


X7 = df_val
# Loop over range of min_samples values and calculate silhouette scores

# Initialize DBSCAN model with current min_samples value
dbscan = DBSCAN(eps=6.0, min_samples=17)
# Fit model to data
labels7 = dbscan.fit(X7)


uq = unique_indices(labels7)
print(uq)
a['Class'] = labels
a['country'] = country_names


a.Class.iloc[uq[-1]] = 'Help Required'

a.Class.iloc[uq[1]] = 'Little help Required'




import plotly.express as px
from plotly.offline import iplot
# create a choropleth map using the plotly express function
fig = px.choropleth(a, locations='country', locationmode='country names', color='Class',
                    color_discrete_map={'Help Required': 'Red',
                                        'No Help Required': 'Green',
                                        'Little help Required': 'Yellow'},
                    title='Nations Requiring Help')

# display the map
iplot(fig)